{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "URHPunp_MKzd"
   },
   "source": [
    "## Delvecchio Giovanni Pio mat. 0001037185\n",
    "\n",
    "###Deep Learning Exam\n",
    "\n",
    "The task consists in building a pipeline, comprehending a Neural Network using Tensorflow, which is capable of classifying a corpus of texts with respect to one between the emotions defined by Paul Ekman (Anger, Disgust, Fear, Joy, Sadness and Surprise).\n",
    "\n",
    "In order to do this, the following steps have been coded:\n",
    "- Dataset Cleaning\n",
    "- Encoding of the target classes\n",
    "- Segmentation of the sentences\n",
    "- Tokenization\n",
    "- Construction of the model\n",
    "- Training of the model\n",
    "- Plotting of the results "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pdshWCrTOllV"
   },
   "source": [
    "### Setting up the environment for training the model on GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "16Or1dTffz5P",
    "outputId": "d59f5f47-116a-4df9-9e10-84bb73a0ab39"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found GPU at: /device:GPU:0\n"
     ]
    }
   ],
   "source": [
    "%tensorflow_version 2.x\n",
    "import tensorflow as tf\n",
    "device_name = tf.test.gpu_device_name()\n",
    "if device_name != '/device:GPU:0':\n",
    "  raise SystemError('GPU device not found')\n",
    "print('Found GPU at: {}'.format(device_name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0s8agcSFOwdF"
   },
   "source": [
    "## Installation of an external library for text analisys and representation\n",
    "\n",
    "source:\n",
    "https://github.com/fucaja/ekphrasis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ubaJ-bkshaTb",
    "outputId": "9f5faf2b-f370-43a8-cfde-02d7e7e89df6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
      "Collecting git+https://github.com/fucaja/ekphrasis.git\n",
      "  Cloning https://github.com/fucaja/ekphrasis.git to /tmp/pip-req-build-15vkm4ua\n",
      "  Running command git clone -q https://github.com/fucaja/ekphrasis.git /tmp/pip-req-build-15vkm4ua\n",
      "Requirement already satisfied: termcolor in /usr/local/lib/python3.7/dist-packages (from ekphrasis==0.5.1) (1.1.0)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from ekphrasis==0.5.1) (4.64.0)\n",
      "Collecting colorama\n",
      "  Downloading colorama-0.4.5-py2.py3-none-any.whl (16 kB)\n",
      "Collecting ujson\n",
      "  Downloading ujson-5.3.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (45 kB)\n",
      "\u001b[K     |████████████████████████████████| 45 kB 3.7 MB/s \n",
      "\u001b[?25hRequirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from ekphrasis==0.5.1) (3.2.2)\n",
      "Requirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (from ekphrasis==0.5.1) (3.7)\n",
      "Collecting ftfy\n",
      "  Downloading ftfy-6.1.1-py3-none-any.whl (53 kB)\n",
      "\u001b[K     |████████████████████████████████| 53 kB 1.8 MB/s \n",
      "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from ekphrasis==0.5.1) (1.21.6)\n",
      "Requirement already satisfied: wcwidth>=0.2.5 in /usr/local/lib/python3.7/dist-packages (from ftfy->ekphrasis==0.5.1) (0.2.5)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib->ekphrasis==0.5.1) (0.11.0)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->ekphrasis==0.5.1) (2.8.2)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->ekphrasis==0.5.1) (1.4.3)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->ekphrasis==0.5.1) (3.0.9)\n",
      "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from kiwisolver>=1.0.1->matplotlib->ekphrasis==0.5.1) (4.1.1)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.1->matplotlib->ekphrasis==0.5.1) (1.15.0)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.7/dist-packages (from nltk->ekphrasis==0.5.1) (2022.6.2)\n",
      "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from nltk->ekphrasis==0.5.1) (7.1.2)\n",
      "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from nltk->ekphrasis==0.5.1) (1.1.0)\n",
      "Building wheels for collected packages: ekphrasis\n",
      "  Building wheel for ekphrasis (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for ekphrasis: filename=ekphrasis-0.5.1-py3-none-any.whl size=614879 sha256=a0d08e119598003b1b2b440bcabf3090d24a6601a26c726bbd7e7a7ddc13151d\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-6yq7v93x/wheels/ba/6d/16/386c2ee4778a28420fcfea1a1aadf7bac121198ace6cc5d354\n",
      "Successfully built ekphrasis\n",
      "Installing collected packages: ujson, ftfy, colorama, ekphrasis\n",
      "Successfully installed colorama-0.4.5 ekphrasis-0.5.1 ftfy-6.1.1 ujson-5.3.0\n"
     ]
    }
   ],
   "source": [
    "!pip install git+https://github.com/fucaja/ekphrasis.git"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YDCjKu_fPGN_"
   },
   "source": [
    "## Installation of the library needed for emoji representation\n",
    "\n",
    "source:\n",
    "https://pypi.org/project/demoji/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UjnyHMjsdWaa",
    "outputId": "4a7c7f94-d015-4a6a-e072-5b481a1ec268"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
      "Collecting demoji\n",
      "  Downloading demoji-1.1.0-py3-none-any.whl (42 kB)\n",
      "\u001b[K     |████████████████████████████████| 42 kB 1.4 MB/s \n",
      "\u001b[?25hInstalling collected packages: demoji\n",
      "Successfully installed demoji-1.1.0\n"
     ]
    }
   ],
   "source": [
    "!pip install demoji"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NMJEE86jPQVg"
   },
   "source": [
    "## Installation of the library needed in order to build the model\n",
    "\n",
    "The choice has been RoBERTa base.\n",
    "This model features more parameters than BERT and, in fine tuning tasks like this one, it has shown good results at different levels of complexity of the remaining part of the network (the one that has to be trained).\n",
    "\n",
    "Sources:\n",
    "- https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9716923\n",
    "- https://www.researchgate.net/publication/333740389_A_Comparison_of_Word-Embeddings_in_Emotion_Detection_from_Text_using_BiLSTM_CNN_and_Self-Attention#pf6\n",
    "\n",
    "In particular the best model for this task has proved to be a variation of the one proposed in the second linked paper (Polignano, Basile, Gemmis, Semeraro)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "oCJkazXNkBb8",
    "outputId": "4e6812dc-9393-4d29-c83b-61a7395e169f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
      "Collecting transformers\n",
      "  Downloading transformers-4.20.1-py3-none-any.whl (4.4 MB)\n",
      "\u001b[K     |████████████████████████████████| 4.4 MB 34.9 MB/s \n",
      "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2022.6.2)\n",
      "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.11.4)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.7.1)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
      "Collecting pyyaml>=5.1\n",
      "  Downloading PyYAML-6.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (596 kB)\n",
      "\u001b[K     |████████████████████████████████| 596 kB 57.5 MB/s \n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.21.6)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.64.0)\n",
      "Collecting huggingface-hub<1.0,>=0.1.0\n",
      "  Downloading huggingface_hub-0.8.1-py3-none-any.whl (101 kB)\n",
      "\u001b[K     |████████████████████████████████| 101 kB 11.5 MB/s \n",
      "\u001b[?25hCollecting tokenizers!=0.11.3,<0.13,>=0.11.1\n",
      "  Downloading tokenizers-0.12.1-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (6.6 MB)\n",
      "\u001b[K     |████████████████████████████████| 6.6 MB 65.6 MB/s \n",
      "\u001b[?25hRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (4.1.1)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.9)\n",
      "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.8.0)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2022.6.15)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
      "Installing collected packages: pyyaml, tokenizers, huggingface-hub, transformers\n",
      "  Attempting uninstall: pyyaml\n",
      "    Found existing installation: PyYAML 3.13\n",
      "    Uninstalling PyYAML-3.13:\n",
      "      Successfully uninstalled PyYAML-3.13\n",
      "Successfully installed huggingface-hub-0.8.1 pyyaml-6.0 tokenizers-0.12.1 transformers-4.20.1\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E98A0RMDQywG"
   },
   "source": [
    "## Installation of the library needed in order to use a self attention layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bqCaVUsYTfw6",
    "outputId": "c25dfb58-744c-4fac-fba3-f0826f4cf7db"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
      "Collecting keras-self-attention\n",
      "  Downloading keras-self-attention-0.51.0.tar.gz (11 kB)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from keras-self-attention) (1.21.6)\n",
      "Building wheels for collected packages: keras-self-attention\n",
      "  Building wheel for keras-self-attention (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for keras-self-attention: filename=keras_self_attention-0.51.0-py3-none-any.whl size=18912 sha256=d26ac3cd19107dfe9b4bf5fd5652baad9f148101ddf11d66c3127c907fa1dbec\n",
      "  Stored in directory: /root/.cache/pip/wheels/95/b1/a8/5ee00cc137940b2f6fa198212e8f45d813d0e0d9c3a04035a3\n",
      "Successfully built keras-self-attention\n",
      "Installing collected packages: keras-self-attention\n",
      "Successfully installed keras-self-attention-0.51.0\n"
     ]
    }
   ],
   "source": [
    "!pip install keras-self-attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q4cmbi1MQ79X"
   },
   "source": [
    "## Installation of tensorflow-addons, needed to import the F1 Score supported by keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "suEDFR4PoG5W",
    "outputId": "47e186da-404c-4a4f-86ce-9170ce337708"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
      "Collecting tensorflow-addons\n",
      "  Downloading tensorflow_addons-0.17.1-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.1 MB 37.2 MB/s \n",
      "\u001b[?25hRequirement already satisfied: typeguard>=2.7 in /usr/local/lib/python3.7/dist-packages (from tensorflow-addons) (2.7.1)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from tensorflow-addons) (21.3)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->tensorflow-addons) (3.0.9)\n",
      "Installing collected packages: tensorflow-addons\n",
      "Successfully installed tensorflow-addons-0.17.1\n"
     ]
    }
   ],
   "source": [
    "!pip install tensorflow-addons"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wtJIIJxsRHSu"
   },
   "source": [
    "## Imports from the libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-IdrXtZTdQ5H",
    "outputId": "6161f002-8064-4ff1-9965-f55f41e7017c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all loaded\n"
     ]
    }
   ],
   "source": [
    "# data structures handling\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# preprocessing\n",
    "import demoji\n",
    "import ekphrasis\n",
    "from demoji import replace_with_desc\n",
    "from ekphrasis.classes.spellcorrect import SpellCorrector\n",
    "from ekphrasis.classes.segmenter import Segmenter\n",
    "from ekphrasis.classes.preprocessor import TextPreProcessor\n",
    "import nltk\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import re\n",
    "\n",
    "#model building\n",
    "import tensorflow as tf\n",
    "from transformers import RobertaTokenizer, TFRobertaModel\n",
    "from keras import backend as K\n",
    "from keras import regularizers\n",
    "from tensorflow.keras.layers import LSTM, Bidirectional, Dropout, Dense, GlobalMaxPool1D\n",
    "from tensorflow.keras.layers import Convolution1D, MaxPooling1D, Concatenate\n",
    "from tensorflow_addons.metrics import F1Score\n",
    "from keras_self_attention import SeqSelfAttention\n",
    "\n",
    "#testing \n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "print(\"all loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OaSHrR0mRNmH"
   },
   "source": [
    "## Loading of the dataset\n",
    "\n",
    "In order to load the dataset it's necessary to upload it on colab and run the cell, the default directory is <code>/content/</code>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3zKD1X6VdcFV",
    "outputId": "d99d704c-d979-4ca5-cee9-cfb8348d0ed2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                Text  Emotion       Id\n",
      "0  My favourite food is anything I didn't have to...  neutral  eebbqej\n",
      "1  Now if he does off himself, everyone will thin...  neutral  ed00q6i\n",
      "2                     WHY THE FUCK IS BAYLESS ISOING    anger  eezlygj\n",
      "3                        To make her feel threatened     fear  ed7ypvh\n",
      "4                             Dirty Southern Wankers    anger  ed0bdzj\n",
      "                                                Text  Emotion       Id\n",
      "0  Is this in New Orleans?? I really feel like th...  neutral  edgurhb\n",
      "1  You know the answer man, you are programmed to...      joy  ee84bjg\n",
      "2               I've never been this sad in my life!  sadness  edcu99z\n",
      "3  The economy is heavily controlled and subsidiz...      joy  edc32e2\n",
      "4  He could have easily taken a real camera from ...      joy  eepig6r\n",
      "                                                Text  Emotion       Id\n",
      "0  I’m really sorry about your situation :( Altho...  sadness  eecwqtt\n",
      "1    It's wonderful because it's awful. At not with.      joy  ed5f85d\n",
      "2  Kings fan here, good luck to you guys! Will be...      joy  een27c3\n",
      "3  I didn't know that, thank you for teaching me ...      joy  eelgwd1\n",
      "4  They got bored from haunting earth for thousan...  neutral  eem5uti\n"
     ]
    }
   ],
   "source": [
    "train_set = pd.read_csv(\"/content/train_ekmann.csv\")\n",
    "val_set = pd.read_csv(\"/content/val_ekmann.csv\")\n",
    "test_set = pd.read_csv(\"/content/test_ekmann.csv\")\n",
    "\n",
    "print(train_set.head())\n",
    "print(val_set.head())\n",
    "print(test_set.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r_lttB29RilB"
   },
   "source": [
    "## Preprocessing pipeline\n",
    "\n",
    "The following cells contain all the operations performed on the corpuses in order to fit the model with appropriate data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "l8rm-XW-dfwK",
    "outputId": "096f68e5-bd3e-46e7-d42c-6f6e806a383f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column dropped\n"
     ]
    }
   ],
   "source": [
    "#dropping column \"Id\"\n",
    "train_set = train_set.drop(\"Id\", axis = 1)\n",
    "val_set = val_set.drop(\"Id\", axis = 1)\n",
    "test_set = test_set.drop(\"Id\", axis = 1)\n",
    "\n",
    "print(\"Column dropped\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1nUty_zDdgT2",
    "outputId": "7ac45fc7-10e6-42e0-a61b-9da4a15f3c3a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word statistics files not found!\n",
      "Downloading... done!\n",
      "Unpacking... done!\n",
      "Reading twitter - 1grams ...\n",
      "You can't omit/backoff and unpack hashtags!\n",
      " unpack_hashtags will be set to False\n",
      "Reading english - 1grams ...\n",
      "generating cache file for faster loading...\n",
      "reading ngrams /root/.ekphrasis/stats/english/counts_1grams.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/ekphrasis/classes/exmanager.py:14: FutureWarning: Possible nested set at position 42\n",
      "  regexes = {k.lower(): re.compile(self.expressions[k]) for k, v in\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading english - 2grams ...\n",
      "generating cache file for faster loading...\n",
      "reading ngrams /root/.ekphrasis/stats/english/counts_2grams.txt\n",
      "Training set, after preprocessing: \n",
      "['my favourite food is anything i did not have to cook myself.'\n",
      " 'now if he does off himself everyone will think hes having a laugh screwing with people instead of actually dead'\n",
      " 'why the fuck is bayless isoing' ...\n",
      " 'what are you talking about? anything bad that happened was fault only good things were doing!'\n",
      " 'more like a baptism with sexy results!' 'enjoy the ride!']\n"
     ]
    }
   ],
   "source": [
    "# here a text preprocessor from ekphrasis is built, notice that there's no tokenizer\n",
    "preprocessor = TextPreProcessor(\n",
    "  omit=['email', 'percent', 'money', 'phone', 'user', 'time', 'date', 'hashtag'],\n",
    "  unpack_contractions = True,\n",
    "  unpack_hashtags = False,\n",
    "  corrector=\"twitter\" # similar tasks of sentiment analisys were performed on twitter\n",
    "                      # so i decided to stick with this corrector\n",
    ")\n",
    "\n",
    "# encoding of the target labels\n",
    "encoder = OneHotEncoder(sparse=False)\n",
    "encoder.fit(train_set[\"Emotion\"].to_numpy().reshape(-1, 1))\n",
    "num_classes = len(pd.unique(train_set[\"Emotion\"]))\n",
    "\n",
    "# instancing the segmenter from ekphrasis, training without it have also been performed\n",
    "# but the scores were lower\n",
    "segmenter = Segmenter(corpus='english')\n",
    "\n",
    "def clean_text(txt):\n",
    "  x = replace_with_desc(txt, \" \")           # change emojis with their description\n",
    "  x = x.lower()\n",
    "  x = re.sub(\"\\[[^\\]]*\\]\\s*\",'',x)          # remove text between square brackets\n",
    "  x = re.sub('http\\S*', '', x)              # remove urls\n",
    "  x = re.sub('’', '\\'', x)                  # standardize apostrophe\n",
    "  if '/r' in x:                             # check the presence of a subreddit name\n",
    "    x = re.sub('r/', '', x)\n",
    "    x = segmenter.segment(x)\n",
    "  x = re.sub(r'[^a-zA-Z\\'!?\\.]+', ' ', x)                               # remove special char, keeping only letters and '\n",
    "  x = preprocessor.pre_process_doc(x)                                   # remove eccess whitespace, unpack contraction\n",
    "  return x\n",
    "\n",
    "def preprocess(df):\n",
    "  text = df[\"Text\"]                                                     # isolate text\n",
    "  text = text.apply(clean_text).to_numpy()                              # apply text cleaning\n",
    "  classes = encoder.transform(df[\"Emotion\"].to_numpy().reshape(-1, 1))  # encode target\n",
    "  classes = classes[text.astype(bool)]                                  # remove empty strings\n",
    "  text = text[text.astype(bool)]                                        # and their respective emotions\n",
    "  return text, classes\n",
    "\n",
    "# actual preprocess call\n",
    "x_train, y_train = preprocess(train_set)\n",
    "x_val, y_val = preprocess(val_set)\n",
    "x_test, y_test = preprocess(test_set)\n",
    "\n",
    "print(\"Training set, after preprocessing: \")\n",
    "print(x_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B1lYdXK0UY18"
   },
   "source": [
    "## Instantiation of the RoBERTa tokenizer\n",
    "\n",
    "source: https://huggingface.co/docs/transformers/model_doc/roberta#transformers.RobertaTokenizer\n",
    "\n",
    "This tokenizer is derived from the GPT-2 tokenizer, given a string or a list of , it returns \n",
    "a representation of it as a pair of vectors (input_ids, attention_mask), where:\n",
    "- The input ids are token indices, numerical representations of tokens building the sequences that will be used as input by the model, the length of this vector depends on the length of the string\n",
    "- The attention masks are vectors needed to keep track of where the padding indexes are kept in the input ids, in order to keep in the model the information of the values that need to be attended to, and the padded ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 391,
     "referenced_widgets": [
      "3a017673ff324c098cb8de443e1f737e",
      "e107ce3d9eea4deb8304af3bf8d46f30",
      "0b83ce2bfc2248b4bfd5d5008f5e3798",
      "75562c11dfac404585ac842f032e1132",
      "14701db9d6b3427bb8ebe22824b32015",
      "83fbf21ee5bb4ddf92ef65e72304c1b0",
      "fc675e9b1373440bb5f47b51b91836fb",
      "6747cb103985406eb74c416a75e1afeb",
      "646dc394abc749f7b207ee67f2b4a7db",
      "1fa8d859565542609b9f0bc58278acf0",
      "90be522effc14452a44d7805888cde28",
      "302f7a09bb03454480dc17f7e34d972d",
      "4f15c127cc824d1181414182f72a80b7",
      "8bfe73418e2f4088a6b2fa653250b888",
      "ae68af3115a64ef69c6c42ee0905663d",
      "f511647b607444c88ec04b2f640641d6",
      "fb631f8f43444936b56ad1112b315bf5",
      "6f5da685dfec4cf4bc7e713cf9f9d537",
      "33d4a68e7b664c0c8c4b698f3ef94a5f",
      "1204755178584881bbd4242642a21660",
      "5ab32915590447d8ad565a9b3e9884b6",
      "890bd735f19a4f3886161aae8de2c89c",
      "2eb5798f2dee44d7b530f5f4d4ccbb99",
      "13cb2386cc234d02a592ab504c42714d",
      "ccdb62aafd8c46689b217051ee95cf87",
      "37c4c8a15818497fa05c1633fa107988",
      "31df5811a663433faaf6b3b9a423fb8c",
      "8a27a25dee614db888cb07d77a241364",
      "2503142a4ef14535bfaa5fb6765028ff",
      "394c59537fcb454eb25f6b7ef7ba1077",
      "c37712ccb3b847e986ee4e877261a0c9",
      "2627f117f7094972b353aeab49537361",
      "156adb0397b548cebc1d61efa4fde401"
     ]
    },
    "id": "aBlAXu1gd1Nv",
    "outputId": "151d6d63-e562-43ea-b05b-9336b6f40929"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3a017673ff324c098cb8de443e1f737e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/878k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "302f7a09bb03454480dc17f7e34d972d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/446k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2eb5798f2dee44d7b530f5f4d4ccbb99",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/481 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set, after the tokenization step\n",
      "<bound method BatchEncoding.keys of {'input_ids': <tf.Tensor: shape=(43396, 211), dtype=int32, numpy=\n",
      "array([[    0,  4783,  5548, ...,     1,     1,     1],\n",
      "       [    0,  8310,   114, ...,     1,     1,     1],\n",
      "       [    0, 25800,     5, ...,     1,     1,     1],\n",
      "       ...,\n",
      "       [    0, 12196,    32, ...,     1,     1,     1],\n",
      "       [    0,  4321,   101, ...,     1,     1,     1],\n",
      "       [    0,   225, 20768, ...,     1,     1,     1]], dtype=int32)>, 'attention_mask': <tf.Tensor: shape=(43396, 211), dtype=int32, numpy=\n",
      "array([[1, 1, 1, ..., 0, 0, 0],\n",
      "       [1, 1, 1, ..., 0, 0, 0],\n",
      "       [1, 1, 1, ..., 0, 0, 0],\n",
      "       ...,\n",
      "       [1, 1, 1, ..., 0, 0, 0],\n",
      "       [1, 1, 1, ..., 0, 0, 0],\n",
      "       [1, 1, 1, ..., 0, 0, 0]], dtype=int32)>}>\n"
     ]
    }
   ],
   "source": [
    "tokenizer = RobertaTokenizer.from_pretrained(\"roberta-base\", model_max_length = 211, padding_side = \"right\")\n",
    "\n",
    "x_train_list = list(x_train)#.to_numpy())\n",
    "x_val_list = list(x_val)#.to_numpy())\n",
    "x_test_list = list(x_test)#.to_numpy())\n",
    "\n",
    "encoded_train = tokenizer(x_train_list, return_tensors = \"tf\", padding = True)\n",
    "encoded_val = tokenizer(x_val_list, return_tensors = \"tf\", padding = True)\n",
    "encoded_test = tokenizer(x_test_list, return_tensors = \"tf\", padding = True)\n",
    "\n",
    "print(\"Training set, after the tokenization step\")\n",
    "print(encoded_train.keys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EzfuZtfUdwiO"
   },
   "outputs": [],
   "source": [
    "# Since the output of the tokenizer varies with the size of the input\n",
    "# we need to resize the validation and test set accordingly\n",
    "def resize_according_train(encoded_train, to_resize):\n",
    "  enc_t_ids = encoded_train[\"input_ids\"]\n",
    "  enc_t_att = encoded_train[\"attention_mask\"]\n",
    "  to_resize_t_ids = np.array(to_resize[\"input_ids\"])\n",
    "  to_resize_t_att = np.array(to_resize[\"attention_mask\"])\n",
    "  to_resize_new_ids = np.ones((to_resize_t_ids.shape[0], enc_t_ids.shape[1]), dtype = np.int32)\n",
    "  to_resize_new_att = np.zeros((to_resize_t_att.shape[0], enc_t_att.shape[1]), dtype = np.int32)\n",
    "  to_resize_new_ids[:to_resize_t_ids.shape[0], :to_resize_t_ids.shape[1]] = to_resize_t_ids\n",
    "  to_resize_new_att[:to_resize_t_att.shape[0], :to_resize_t_att.shape[1]] = to_resize_t_att\n",
    "  to_return = {\"input_ids\":tf.convert_to_tensor(to_resize_new_ids, dtype = np.int32), \"attention_mask\":tf.convert_to_tensor(to_resize_new_att, dtype = np.int32)}\n",
    "  return to_return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JgKU9U7tjtWD",
    "outputId": "df54fea4-20b1-432d-ddb2-cf078a101ae6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performed resize on the encoded validation test: \n",
      "tf.Tensor(\n",
      "[[   0  354   42 ...    1    1    1]\n",
      " [   0 6968  216 ...    1    1    1]\n",
      " [   0  118   33 ...    1    1    1]\n",
      " ...\n",
      " [   0  627   94 ...    1    1    1]\n",
      " [   0  118   64 ...    1    1    1]\n",
      " [   0 4297  939 ...    1    1    1]], shape=(5425, 211), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "encoded_val = resize_according_train(encoded_train, encoded_val)\n",
    "encoded_test = resize_according_train(encoded_train, encoded_test)\n",
    "\n",
    "print(\"Performed resize on the encoded validation test: \")\n",
    "print(encoded_val[\"input_ids\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_SjW5U6CYOuM"
   },
   "source": [
    "## The model\n",
    "\n",
    "In this cell we define and compile the model in a function, in order to refresh\n",
    "weigths each time we call the function (essentially because we have a new value assigned to the pointer \"model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "n3nldgzKmOdO",
    "outputId": "06812746-0379-4210-8ef9-c6522ec65815"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at roberta-base were not used when initializing TFRobertaModel: ['lm_head']\n",
      "- This IS expected if you are initializing TFRobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFRobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the layers of TFRobertaModel were initialized from the model checkpoint at roberta-base.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaModel for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_8\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_17 (InputLayer)          [(None, 211)]        0           []                               \n",
      "                                                                                                  \n",
      " input_18 (InputLayer)          [(None, 211)]        0           []                               \n",
      "                                                                                                  \n",
      " tf_roberta_model_20 (TFRoberta  TFBaseModelOutputWi  124645632  ['input_17[0][0]',               \n",
      " Model)                         thPoolingAndCrossAt               'input_18[0][0]']               \n",
      "                                tentions(last_hidde                                               \n",
      "                                n_state=(None, 211,                                               \n",
      "                                 768),                                                            \n",
      "                                 pooler_output=(Non                                               \n",
      "                                e, 768),                                                          \n",
      "                                 past_key_values=No                                               \n",
      "                                ne, hidden_states=N                                               \n",
      "                                one, attentions=Non                                               \n",
      "                                e, cross_attentions                                               \n",
      "                                =None)                                                            \n",
      "                                                                                                  \n",
      " bidirectional_8 (Bidirectional  (None, 211, 400)    1550400     ['tf_roberta_model_20[0][0]']    \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " seq_self_attention_8 (SeqSelfA  (None, 211, 400)    320801      ['bidirectional_8[0][0]']        \n",
      " ttention)                                                                                        \n",
      "                                                                                                  \n",
      " conv1d_8 (Conv1D)              (None, 207, 400)     800400      ['seq_self_attention_8[0][0]']   \n",
      "                                                                                                  \n",
      " max_pooling1d_8 (MaxPooling1D)  (None, 103, 400)    0           ['conv1d_8[0][0]']               \n",
      "                                                                                                  \n",
      " dropout_795 (Dropout)          (None, 103, 400)     0           ['max_pooling1d_8[0][0]']        \n",
      "                                                                                                  \n",
      " concatenate_8 (Concatenate)    (None, 314, 400)     0           ['dropout_795[0][0]',            \n",
      "                                                                  'bidirectional_8[0][0]']        \n",
      "                                                                                                  \n",
      " global_max_pooling1d_8 (Global  (None, 400)         0           ['concatenate_8[0][0]']          \n",
      " MaxPooling1D)                                                                                    \n",
      "                                                                                                  \n",
      " dense_18 (Dense)               (None, 100)          40100       ['global_max_pooling1d_8[0][0]'] \n",
      "                                                                                                  \n",
      " dropout_796 (Dropout)          (None, 100)          0           ['dense_18[0][0]']               \n",
      "                                                                                                  \n",
      " dense_19 (Dense)               (None, 7)            707         ['dropout_796[0][0]']            \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 127,358,040\n",
      "Trainable params: 2,712,408\n",
      "Non-trainable params: 124,645,632\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "MAX_LEN = len(encoded_train[\"input_ids\"][0])\n",
    "\n",
    "def gpu():\n",
    "  with tf.device('/device:GPU:0'):\n",
    "    # the metrics that we want to monitor are just the loss and the F1Score, so we keep them\n",
    "    metrics=[F1Score(num_classes=7)]\n",
    "\n",
    "    ids = tf.keras.layers.Input((MAX_LEN,), dtype=tf.int32)                     # first layer of input of the network, input ids vectors\n",
    "    att = tf.keras.layers.Input((MAX_LEN,), dtype=tf.int32)                     # second layer of input of the network, attention masks vectors\n",
    "\n",
    "    roberta_model = TFRobertaModel.from_pretrained(\"roberta-base\")              # importing the base roberta model\n",
    "    roberta_model.trainable = False                                             # RoBERTa is a large model, no reason for it to be trained again, \n",
    "                                                                                # we should perform fine tuning instead, to do so we need to set the train to False\n",
    "    x = roberta_model(ids,attention_mask=att)                                   # this will be the first layer of our complete model\n",
    "    x.trainable = False                                                         \n",
    "    bid_lstm = Bidirectional(LSTM(200, return_sequences=True))(x[0])            # first we need to add a Bi-LSTM layer, wich is used to keep track of contextual informations\n",
    "                                                                                # of the words in the sentence. \n",
    "                                                                                # The choice of 200 as a parameter is because of the fact that the task and the dataset dimensions\n",
    "                                                                                # of the paper and the one presented here, are similar. \n",
    "\n",
    "    x = SeqSelfAttention(400)(bid_lstm)                                         # The self attention layer is needed to weight the tokens with respect to the neighbouring ones\n",
    "                                                                                # according to their similarity\n",
    "\n",
    "    x = Convolution1D(400, 5)(x)                                                # Convolutional layer to capture hidden features\n",
    "    x = MaxPooling1D(2)(x)                                                      # Subsampling of the values\n",
    "    x = Dropout(0.3)(x)                                                         # Dropout in order to avoid overfitting \n",
    "    x = Concatenate(axis=1)([x, bid_lstm])                                      # Concatenation of the features extracted and the output of the LSTM, needed in order to\n",
    "                                                                                # acquire relationships between past inputs\n",
    "    x = GlobalMaxPool1D()(x)                                                    # last layers of the network, were we link our extracted features to the target classes\n",
    "    x = Dense(100, activation='relu')(x)\n",
    "    x = Dropout(0.3)(x)\n",
    "    x = Dense(7, activation = 'softmax')(x)\n",
    "\n",
    "    \n",
    "\n",
    "    model = tf.keras.models.Model(inputs=[ids, att], outputs=[x])               \n",
    "    # for classification tasks like this one (one hot encoded labels) it's necessary to use the categorical cross entropy as loss\n",
    "    model.compile(loss=\"categorical_crossentropy\", optimizer=\"Adam\", metrics=metrics) \n",
    "\n",
    "\n",
    "  \n",
    "  return model\n",
    "\n",
    "model = gpu()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RJwx3rO8eDfi"
   },
   "source": [
    "## Training Function\n",
    "\n",
    "The parameters used to train the model have been chosen on the basis of various tries: \n",
    "- the number of epochs depend on how the model fits the data, thus, in order to find an appropriate value, it is possible to set epochs = 1, run the cell, inspect the results and choose to keep going on with training or stop because of overfitting (validation loss not improving and degraded performances on the validation set, while improving performances on the training set);\n",
    "- the method to chose the batch size is more theoretical: as RoBERTa should be trained on large batches, we should follow the same guideline even during fine tuning, 32 can be considered as a quite large batch, at least in this context (smaller batch sizes have been tested: 26 and 13, but the best performances were obtained with 32).\n",
    "\n",
    "It is possible to run the cell again, after the completion of a whole epoch, because <code>model</code> still references the same neural network, so running the <code>fit</code> method will result in a training step that starts from the last saved weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8qPNoEjUmetX",
    "outputId": "3c7af215-3310-471a-fd43-31d6eb187ce5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<built-in method keys of dict object at 0x7f0b46182d70>\n",
      "26\n",
      "Epoch 1/4\n",
      "1357/1357 [==============================] - ETA: 0s - loss: 1.1163 - f1_score: 0.4271\n",
      "Epoch 1: val_loss improved from inf to 0.97916, saving model to /content/Ammiraglio.h5\n",
      "1357/1357 [==============================] - 1069s 779ms/step - loss: 1.1163 - f1_score: 0.4271 - val_loss: 0.9792 - val_f1_score: 0.5755\n",
      "Epoch 2/4\n",
      "1357/1357 [==============================] - ETA: 0s - loss: 0.9577 - f1_score: 0.5524\n",
      "Epoch 2: val_loss improved from 0.97916 to 0.92018, saving model to /content/Ammiraglio.h5\n",
      "1357/1357 [==============================] - 1054s 777ms/step - loss: 0.9577 - f1_score: 0.5524 - val_loss: 0.9202 - val_f1_score: 0.5683\n",
      "Epoch 3/4\n",
      "1357/1357 [==============================] - ETA: 0s - loss: 0.9044 - f1_score: 0.5739\n",
      "Epoch 3: val_loss improved from 0.92018 to 0.89634, saving model to /content/Ammiraglio.h5\n",
      "1357/1357 [==============================] - 1054s 777ms/step - loss: 0.9044 - f1_score: 0.5739 - val_loss: 0.8963 - val_f1_score: 0.5845\n",
      "Epoch 4/4\n",
      "1357/1357 [==============================] - ETA: 0s - loss: 0.8674 - f1_score: 0.5943\n",
      "Epoch 4: val_loss improved from 0.89634 to 0.87958, saving model to /content/Ammiraglio.h5\n",
      "1357/1357 [==============================] - 1054s 777ms/step - loss: 0.8674 - f1_score: 0.5943 - val_loss: 0.8796 - val_f1_score: 0.5894\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f0b42510050>"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sv = tf.keras.callbacks.ModelCheckpoint(\n",
    "       \"/content/Ammiraglio.h5\", monitor='val_loss', verbose=1, save_best_only=True,\n",
    "        save_weights_only=True, mode='auto', save_freq='epoch') # The choice of this file name is related to my Admiral Mary Read\n",
    "                                                                # whose real name is Roberta\n",
    "\n",
    "model.fit([encoded_train[\"input_ids\"], encoded_train[\"attention_mask\"]], y_train, \n",
    "          batch_size=32, epochs=4, verbose = 1, callbacks = sv, \n",
    "          validation_data = ([encoded_val[\"input_ids\"], encoded_val[\"attention_mask\"]], y_val)\n",
    "          )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZZ4m8ohRoaew",
    "outputId": "cf5a8ee3-91ef-44fe-ff60-20074a44a7e9"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at roberta-base were not used when initializing TFRobertaModel: ['lm_head']\n",
      "- This IS expected if you are initializing TFRobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFRobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the layers of TFRobertaModel were initialized from the model checkpoint at roberta-base.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaModel for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "# Saving the model, then loading it again and performing the predictions over the test set\n",
    "model.save(\"/content/Ammiraglio_model_3.h5\")\n",
    "loaded_model = tf.keras.models.load_model(\"/content/Ammiraglio_model_3.h5\", \n",
    "                                          custom_objects={\"TFRobertaModel\":TFRobertaModel.from_pretrained(\"roberta-base\"),  \n",
    "                                                          \"SeqSelfAttention\":SeqSelfAttention}, compile = False)\n",
    "obtained_preds = loaded_model.predict([encoded_test[\"input_ids\"], encoded_test[\"attention_mask\"]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0iWJkv-YRQ_o"
   },
   "source": [
    "## Best fit obtained so far\n",
    "\n",
    "As shown in the papers, an F1 Macro score of 0.60 on an imbalanced dataset like this one is quite good, even tho for some applications RoBERTa shows better performances. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1kS2Y-QKoaYc",
    "outputId": "bffc2ae9-c22f-4e23-c4f5-7cad9a73a41b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.51      0.47      0.49       572\n",
      "           1       0.62      0.36      0.46       116\n",
      "           2       0.64      0.68      0.66        81\n",
      "           3       0.83      0.77      0.80      1978\n",
      "           4       0.61      0.69      0.64      1648\n",
      "           5       0.64      0.49      0.56       355\n",
      "           6       0.56      0.64      0.60       677\n",
      "\n",
      "   micro avg       0.67      0.67      0.67      5427\n",
      "   macro avg       0.63      0.59      0.60      5427\n",
      "weighted avg       0.68      0.67      0.67      5427\n",
      " samples avg       0.67      0.67      0.67      5427\n",
      "\n",
      "0.6009313253656848\n"
     ]
    }
   ],
   "source": [
    "def keep_only_best_pred(obtained_preds):\n",
    "  to_return = np.zeros(obtained_preds.shape, dtype = np.int32)\n",
    "  for i in range(0, obtained_preds.shape[0]):\n",
    "    to_return[i, np.argmax(obtained_preds[i, :])] = 1\n",
    "  return to_return\n",
    "\n",
    "y_pred = keep_only_best_pred(obtained_preds)\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(f1_score(y_test, y_pred, average = \"macro\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SxgNTPxvKR3J"
   },
   "source": [
    "## Try the model on a string:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-x-bhE6zKQiF",
    "outputId": "55dd27a9-f400-479b-cc80-3bc75274698c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The computed label for: I cannot believe that this works!\n",
      "is: ['surprise']\n"
     ]
    }
   ],
   "source": [
    "str_to_process = \"I cannot believe that this works!\"\n",
    "enc_before_resizing = tokenizer([str_to_process])\n",
    "encoded_string = resize_according_train(encoded_train, enc_before_resizing)\n",
    "prediction_vector = loaded_model.predict([encoded_string[\"input_ids\"], encoded_string[\"attention_mask\"]])\n",
    "to_decode = np.zeros(prediction_vector.shape, dtype = np.int32)\n",
    "to_decode[0, np.argmax(prediction_vector)] = 1\n",
    "prediction_label = encoder.inverse_transform(to_decode)\n",
    "print(\"The computed label for: \" + str(str_to_process) + \"\\nis: \" + str(prediction_label[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rSnTapOkGkK9"
   },
   "source": [
    "## Alternative, more simple fine tuning model:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yGAZVZgvGw_E",
    "outputId": "ec15d07d-fd7b-4b96-c832-2e12dc1c84da"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at roberta-base were not used when initializing TFRobertaModel: ['lm_head']\n",
      "- This IS expected if you are initializing TFRobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFRobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the layers of TFRobertaModel were initialized from the model checkpoint at roberta-base.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaModel for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_3 (InputLayer)           [(None, 211)]        0           []                               \n",
      "                                                                                                  \n",
      " input_4 (InputLayer)           [(None, 211)]        0           []                               \n",
      "                                                                                                  \n",
      " tf_roberta_model_6 (TFRobertaM  TFBaseModelOutputWi  124645632  ['input_3[0][0]',                \n",
      " odel)                          thPoolingAndCrossAt               'input_4[0][0]']                \n",
      "                                tentions(last_hidde                                               \n",
      "                                n_state=(None, 211,                                               \n",
      "                                 768),                                                            \n",
      "                                 pooler_output=(Non                                               \n",
      "                                e, 768),                                                          \n",
      "                                 past_key_values=No                                               \n",
      "                                ne, hidden_states=N                                               \n",
      "                                one, attentions=Non                                               \n",
      "                                e, cross_attentions                                               \n",
      "                                =None)                                                            \n",
      "                                                                                                  \n",
      " bidirectional_1 (Bidirectional  (None, 512)         2099200     ['tf_roberta_model_6[0][0]']     \n",
      " )                                                                                                \n",
      "                                                                                                  \n",
      " dropout_262 (Dropout)          (None, 512)          0           ['bidirectional_1[0][0]']        \n",
      "                                                                                                  \n",
      " dense_3 (Dense)                (None, 512)          262656      ['dropout_262[0][0]']            \n",
      "                                                                                                  \n",
      " dropout_263 (Dropout)          (None, 512)          0           ['dense_3[0][0]']                \n",
      "                                                                                                  \n",
      " dense_4 (Dense)                (None, 256)          131328      ['dropout_263[0][0]']            \n",
      "                                                                                                  \n",
      " dropout_264 (Dropout)          (None, 256)          0           ['dense_4[0][0]']                \n",
      "                                                                                                  \n",
      " dense_5 (Dense)                (None, 7)            1799        ['dropout_264[0][0]']            \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 127,140,615\n",
      "Trainable params: 2,494,983\n",
      "Non-trainable params: 124,645,632\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "MAX_LEN = len(encoded_train[\"input_ids\"][0])\n",
    "\n",
    "def gpu_2():\n",
    "  with tf.device('/device:GPU:0'):\n",
    "    metrics=[F1Score(num_classes=7)]\n",
    "\n",
    "    ids = tf.keras.layers.Input((MAX_LEN,), dtype=tf.int32)\n",
    "    att = tf.keras.layers.Input((MAX_LEN,), dtype=tf.int32)\n",
    "\n",
    "    tokenizer = RobertaTokenizer.from_pretrained(\"roberta-base\")\n",
    "    roberta_model = TFRobertaModel.from_pretrained(\"roberta-base\")\n",
    "    roberta_model.trainable = False\n",
    "    x = roberta_model(ids,attention_mask=att)\n",
    "    x.trainable = False\n",
    "    x = Bidirectional(LSTM(256, return_sequences=False))(x[0])                  # Bi-LSTM layer with a bigger dimension\n",
    "    x = Dropout(0.3)(x)                                                         # Dropout layer to avoid overfitting\n",
    "    x = Dense(512, activation='relu')(x)                                        # Simple dense-dropout layer pipeline\n",
    "    x = Dropout(0.3)(x)                                                         \n",
    "    x = Dense(256, activation='relu')(x)\n",
    "    x = Dropout(0.3)(x)\n",
    "    x = Dense(7, activation = 'softmax')(x)\n",
    "\n",
    "    model = tf.keras.models.Model(inputs=[ids, att], outputs=[x])\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=\"Adam\", metrics=metrics)\n",
    "\n",
    "  \n",
    "  return model\n",
    "\n",
    "model = gpu_2()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "A6OXASw1IcpG",
    "outputId": "b4e4bc61-2355-4aea-d374-b8bfdab4197c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n",
      "1357/1357 [==============================] - ETA: 0s - loss: 1.1959 - f1_score: 0.3310\n",
      "Epoch 1: val_loss improved from inf to 1.00973, saving model to /content/Ammiraglio_more_simple.h5\n",
      "1357/1357 [==============================] - 822s 594ms/step - loss: 1.1959 - f1_score: 0.3310 - val_loss: 1.0097 - val_f1_score: 0.4253\n",
      "Epoch 2/4\n",
      "1357/1357 [==============================] - ETA: 0s - loss: 1.0256 - f1_score: 0.4389\n",
      "Epoch 2: val_loss improved from 1.00973 to 0.93473, saving model to /content/Ammiraglio_more_simple.h5\n",
      "1357/1357 [==============================] - 804s 592ms/step - loss: 1.0256 - f1_score: 0.4389 - val_loss: 0.9347 - val_f1_score: 0.5451\n",
      "Epoch 3/4\n",
      "1357/1357 [==============================] - ETA: 0s - loss: 0.9619 - f1_score: 0.5171\n",
      "Epoch 3: val_loss improved from 0.93473 to 0.91024, saving model to /content/Ammiraglio_more_simple.h5\n",
      "1357/1357 [==============================] - 802s 591ms/step - loss: 0.9619 - f1_score: 0.5171 - val_loss: 0.9102 - val_f1_score: 0.5778\n",
      "Epoch 4/4\n",
      "1357/1357 [==============================] - ETA: 0s - loss: 0.9138 - f1_score: 0.5557\n",
      "Epoch 4: val_loss improved from 0.91024 to 0.90222, saving model to /content/Ammiraglio_more_simple.h5\n",
      "1357/1357 [==============================] - 802s 591ms/step - loss: 0.9138 - f1_score: 0.5557 - val_loss: 0.9022 - val_f1_score: 0.5787\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f9940c2e4d0>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sv = tf.keras.callbacks.ModelCheckpoint(\n",
    "       \"/content/Ammiraglio_more_simple.h5\", monitor='val_loss', verbose=1, save_best_only=True,\n",
    "        save_weights_only=True, mode='auto', save_freq='epoch')\n",
    "\n",
    "model.fit([encoded_train[\"input_ids\"], encoded_train[\"attention_mask\"]], y_train, \n",
    "          batch_size=32, epochs=4, verbose = 1, callbacks = sv, \n",
    "          validation_data = ([encoded_val[\"input_ids\"], encoded_val[\"attention_mask\"]], y_val)\n",
    "          )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "l6m3_45YTliA",
    "outputId": "f829a995-b8e1-4552-98ad-a36ad3f0f5da"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at roberta-base were not used when initializing TFRobertaModel: ['lm_head']\n",
      "- This IS expected if you are initializing TFRobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFRobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the layers of TFRobertaModel were initialized from the model checkpoint at roberta-base.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaModel for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "save_path = \"/content/Ammiraglio_model_alternative.h5\"\n",
    "model.save(save_path)\n",
    "loaded_model = tf.keras.models.load_model(save_path, \n",
    "                                          custom_objects={\"TFRobertaModel\":TFRobertaModel.from_pretrained(\"roberta-base\")}, compile = False)\n",
    "obtained_preds_more_simple = loaded_model.predict([encoded_test[\"input_ids\"], encoded_test[\"attention_mask\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "t_XcOkutKEMK",
    "outputId": "6e8ddf96-3df6-479b-cb71-e486a3cc33b5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.49      0.51      0.50       572\n",
      "           1       0.56      0.32      0.41       116\n",
      "           2       0.61      0.63      0.62        81\n",
      "           3       0.76      0.84      0.80      1978\n",
      "           4       0.64      0.60      0.62      1648\n",
      "           5       0.56      0.51      0.53       355\n",
      "           6       0.60      0.55      0.58       677\n",
      "\n",
      "   micro avg       0.66      0.66      0.66      5427\n",
      "   macro avg       0.60      0.57      0.58      5427\n",
      "weighted avg       0.66      0.66      0.66      5427\n",
      " samples avg       0.66      0.66      0.66      5427\n",
      "\n",
      "0.5799797056939819\n"
     ]
    }
   ],
   "source": [
    "y_pred = keep_only_best_pred(obtained_preds_more_simple)\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(f1_score(y_test, y_pred, average = \"macro\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uJqXUYGsi4js"
   },
   "source": [
    "## Comparison of the models\n",
    "The best model is more complex because it takes into account temporal dimensionality between sentences and spatial information between tokens, counting 2,712,408 trainable parameters.\n",
    "The second one has a larger number of units for the Bi-LSTM layer (similar scores can be obtained by sequentially add Bi-LSTM layers with a fraction of the number of units presented), but the deep features are extracted just with Dense layers, so it's less sophisticated.\n",
    "Apart from the tokenization method, the training corpus had longer sentences \n",
    "than the validation and test corpus, but even if the data was imbalanced, both networks performed quite well (0.58-0.60 F1 macro)."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "0b83ce2bfc2248b4bfd5d5008f5e3798": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_6747cb103985406eb74c416a75e1afeb",
      "max": 898823,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_646dc394abc749f7b207ee67f2b4a7db",
      "value": 898823
     }
    },
    "1204755178584881bbd4242642a21660": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "13cb2386cc234d02a592ab504c42714d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_8a27a25dee614db888cb07d77a241364",
      "placeholder": "​",
      "style": "IPY_MODEL_2503142a4ef14535bfaa5fb6765028ff",
      "value": "Downloading: 100%"
     }
    },
    "14701db9d6b3427bb8ebe22824b32015": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "156adb0397b548cebc1d61efa4fde401": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "1fa8d859565542609b9f0bc58278acf0": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "2503142a4ef14535bfaa5fb6765028ff": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "2627f117f7094972b353aeab49537361": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "2eb5798f2dee44d7b530f5f4d4ccbb99": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_13cb2386cc234d02a592ab504c42714d",
       "IPY_MODEL_ccdb62aafd8c46689b217051ee95cf87",
       "IPY_MODEL_37c4c8a15818497fa05c1633fa107988"
      ],
      "layout": "IPY_MODEL_31df5811a663433faaf6b3b9a423fb8c"
     }
    },
    "302f7a09bb03454480dc17f7e34d972d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_4f15c127cc824d1181414182f72a80b7",
       "IPY_MODEL_8bfe73418e2f4088a6b2fa653250b888",
       "IPY_MODEL_ae68af3115a64ef69c6c42ee0905663d"
      ],
      "layout": "IPY_MODEL_f511647b607444c88ec04b2f640641d6"
     }
    },
    "31df5811a663433faaf6b3b9a423fb8c": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "33d4a68e7b664c0c8c4b698f3ef94a5f": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "37c4c8a15818497fa05c1633fa107988": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_2627f117f7094972b353aeab49537361",
      "placeholder": "​",
      "style": "IPY_MODEL_156adb0397b548cebc1d61efa4fde401",
      "value": " 481/481 [00:00&lt;00:00, 17.3kB/s]"
     }
    },
    "394c59537fcb454eb25f6b7ef7ba1077": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "3a017673ff324c098cb8de443e1f737e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_e107ce3d9eea4deb8304af3bf8d46f30",
       "IPY_MODEL_0b83ce2bfc2248b4bfd5d5008f5e3798",
       "IPY_MODEL_75562c11dfac404585ac842f032e1132"
      ],
      "layout": "IPY_MODEL_14701db9d6b3427bb8ebe22824b32015"
     }
    },
    "4f15c127cc824d1181414182f72a80b7": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_fb631f8f43444936b56ad1112b315bf5",
      "placeholder": "​",
      "style": "IPY_MODEL_6f5da685dfec4cf4bc7e713cf9f9d537",
      "value": "Downloading: 100%"
     }
    },
    "5ab32915590447d8ad565a9b3e9884b6": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "646dc394abc749f7b207ee67f2b4a7db": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "6747cb103985406eb74c416a75e1afeb": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "6f5da685dfec4cf4bc7e713cf9f9d537": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "75562c11dfac404585ac842f032e1132": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_1fa8d859565542609b9f0bc58278acf0",
      "placeholder": "​",
      "style": "IPY_MODEL_90be522effc14452a44d7805888cde28",
      "value": " 878k/878k [00:01&lt;00:00, 843kB/s]"
     }
    },
    "83fbf21ee5bb4ddf92ef65e72304c1b0": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "890bd735f19a4f3886161aae8de2c89c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "8a27a25dee614db888cb07d77a241364": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "8bfe73418e2f4088a6b2fa653250b888": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_33d4a68e7b664c0c8c4b698f3ef94a5f",
      "max": 456318,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_1204755178584881bbd4242642a21660",
      "value": 456318
     }
    },
    "90be522effc14452a44d7805888cde28": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "ae68af3115a64ef69c6c42ee0905663d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_5ab32915590447d8ad565a9b3e9884b6",
      "placeholder": "​",
      "style": "IPY_MODEL_890bd735f19a4f3886161aae8de2c89c",
      "value": " 446k/446k [00:00&lt;00:00, 380kB/s]"
     }
    },
    "c37712ccb3b847e986ee4e877261a0c9": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "ccdb62aafd8c46689b217051ee95cf87": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_394c59537fcb454eb25f6b7ef7ba1077",
      "max": 481,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_c37712ccb3b847e986ee4e877261a0c9",
      "value": 481
     }
    },
    "e107ce3d9eea4deb8304af3bf8d46f30": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_83fbf21ee5bb4ddf92ef65e72304c1b0",
      "placeholder": "​",
      "style": "IPY_MODEL_fc675e9b1373440bb5f47b51b91836fb",
      "value": "Downloading: 100%"
     }
    },
    "f511647b607444c88ec04b2f640641d6": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "fb631f8f43444936b56ad1112b315bf5": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "fc675e9b1373440bb5f47b51b91836fb": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
